# @package _global_
defaults:
  - override /training: standard
  - override /peft: lora_16

# DoRA vs LoRA comparison experiment
model:
  use_dora: true

training:
  epochs: 3
  lr: 2e-5

dataset:
  dataset_split: "train[:10%]"

output:
  experiment_name: dora-comparison

logging:
  report_to: tensorboard
  project_name: llama-dora-default