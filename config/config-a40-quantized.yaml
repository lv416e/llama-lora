defaults:
  - inference: latest
  - _self_

# A40 GPU with quantization optimization
# Uses 8-bit quantization for maximum memory efficiency

model:
  model_id: meta-llama/Llama-3.2-1B-Instruct
  use_dora: false
  seq_len: 384  # Balanced sequence length
  # Enable quantization settings
  quantization:
    load_in_8bit: true
    load_in_4bit: false  # Can be enabled for even more memory savings

dataset:
  dataset_id: izumi-lab/llm-japanese-dataset
  dataset_split: "train[:1%]"
  val_ratio: 0.1

training:
  lr: 3e-5  # Slightly higher LR for quantized training
  batch_size: 96  # Large batch size with quantization
  gradient_accumulation_steps: 1  # No accumulation needed with large batch
  epochs: 2
  seed: 42
  eval_steps: 300
  early_stopping_patience: 3
  # Training optimizations
  dataloader_pin_memory: true
  dataloader_num_workers: 4

peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - q_proj
    - v_proj
    - o_proj
    - gate_proj  # Added one more module for better performance

output:
  base_output_dir: ./outputs
  experiment_name: a40-quantized
  # All other paths will be auto-generated in the structured format:
  # ./outputs/experiments/{experiment_name}/runs/{run_id}/
  #   ├── artifacts/adapter/
  #   ├── artifacts/tokenizer/
  #   ├── artifacts/merged/
  #   ├── logs/
  #   └── metadata/

logging:
  report_to: tensorboard
  project_name: llama-lora-a40-quantized