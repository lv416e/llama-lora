# LLaMA-LoRA Architecture Overview

## ğŸ—ï¸ System Architecture

### High-Level Component Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA-LoRA Framework                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Configuration Layer (Hydra + Pydantic)                    â”‚
â”‚  â”œâ”€â”€ config/config.yaml (base configuration)               â”‚
â”‚  â”œâ”€â”€ config/schema.py (validation schemas)                 â”‚
â”‚  â””â”€â”€ config/experiment/ (experiment presets)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Pipeline Modules                                      â”‚
â”‚  â”œâ”€â”€ train.py (PEFT training pipeline)                     â”‚
â”‚  â”œâ”€â”€ infer.py (inference with adapters)                    â”‚
â”‚  â”œâ”€â”€ merge.py (adapter integration)                        â”‚
â”‚  â”œâ”€â”€ baseline.py (base model evaluation)                   â”‚
â”‚  â”œâ”€â”€ validate.py (configuration validation)                â”‚
â”‚  â””â”€â”€ experiment.py (multi-experiment runner)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Utility Layer                                             â”‚
â”‚  â”œâ”€â”€ utils/common.py (device, tokenizer, path management)  â”‚
â”‚  â””â”€â”€ utils/exceptions.py (custom error handling)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  External Dependencies                                      â”‚
â”‚  â”œâ”€â”€ PyTorch (core ML framework)                           â”‚
â”‚  â”œâ”€â”€ Transformers (model loading & tokenization)          â”‚
â”‚  â”œâ”€â”€ PEFT (LoRA/DoRA implementation)                       â”‚
â”‚  â””â”€â”€ Accelerate (device optimization)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Core Components Deep Dive

### 1. Configuration Management System
**Hydra + Pydantic Dual Architecture**
```python
# Hydra: Runtime configuration management
@hydra.main(config_path="../../config", config_name="config")
def main(cfg: DictConfig) -> None:
    # Runtime configuration with overrides

# Pydantic: Type validation and safety
class TrainingConfig(BaseModel):
    lr: float = Field(gt=0.0, description="Learning rate")
    batch_size: int = Field(ge=1, description="Batch size")
```

**Configuration Flow:**
1. Base YAML config loaded by Hydra
2. CLI overrides applied (`training.lr=1e-5`)
3. Converted to Pydantic for validation
4. Used throughout pipeline with type safety

### 2. Training Pipeline (train.py)
**Modular Training Architecture**
```python
def main(cfg: DictConfig) -> None:
    validate_and_log_config(cfg)           # Pydantic validation
    device = DeviceManager.detect_device() # Auto device detection
    tokenizer = load_and_setup_tokenizer()  # HF tokenizer setup
    datasets = load_and_process_dataset()   # Alpaca formatting
    model = load_and_setup_model()          # PEFT model creation
    trainer = Trainer(...)                  # HF Trainer integration
    trainer.train()                         # Training execution
    save_artifacts()                        # Adapter persistence
```

**Key Features:**
- **Auto Device Detection**: CUDA â†’ MPS â†’ CPU fallback
- **Memory Optimization**: Gradient checkpointing, mixed precision
- **Flash Attention**: Automatic FlashAttention2 with eager fallback
- **PEFT Integration**: LoRA/DoRA configuration with target module selection

### 3. Inference System (infer.py)
**Adapter Composition Architecture**
```python
# Two-stage model loading
base_model = AutoModelForCausalLM.from_pretrained(model_id)
peft_model = PeftModel.from_pretrained(base_model, adapter_dir)

# Dynamic inference with parameters
response = generate_response(
    model=peft_model,
    prompt=args.prompt,
    max_new_tokens=args.max_new_tokens,
    temperature=args.temperature
)
```

**Inference Flow:**
1. Load base model with optimization flags
2. Load and attach PEFT adapter
3. Configure generation parameters
4. Execute inference with error handling

### 4. Model Merging System (merge.py)
**Adapter Integration Architecture**
```python
# Physical model integration
peft_model = PeftModel.from_pretrained(base_model, adapter_dir)
merged_model = peft_model.merge_and_unload()  # Adapter â†’ base weights
merged_model.save_pretrained(merged_dir)      # Standalone model
```

**Merge Benefits:**
- **Deployment Simplicity**: Single model directory
- **Performance**: No adapter overhead during inference
- **Compatibility**: Standard HuggingFace model format

## ğŸ›ï¸ PEFT (LoRA/DoRA) Implementation

### LoRA Configuration Architecture
```python
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=cfg.peft.r,                    # Rank (8-32 typical)
    lora_alpha=cfg.peft.lora_alpha,  # Scaling factor
    lora_dropout=cfg.peft.lora_dropout,
    target_modules=[               # LLaMA-specific targets
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # MLP
    ],
    use_dora=cfg.model.use_dora,   # DoRA weight decomposition
)
```

### Target Module Strategy for LLaMA
```
LLaMA-3.2 Architecture Targeting:
â”œâ”€â”€ Attention Layers (4 modules)
â”‚   â”œâ”€â”€ q_proj: Query projection
â”‚   â”œâ”€â”€ k_proj: Key projection  
â”‚   â”œâ”€â”€ v_proj: Value projection
â”‚   â””â”€â”€ o_proj: Output projection
â””â”€â”€ MLP Layers (3 modules)
    â”œâ”€â”€ gate_proj: SwiGLU gate mechanism
    â”œâ”€â”€ up_proj: Up-sampling projection
    â””â”€â”€ down_proj: Down-sampling projection
```

## ğŸ–¥ï¸ Device Optimization Architecture

### Multi-Device Support System
```python
class DeviceManager:
    @staticmethod
    def detect_device() -> str:
        if torch.cuda.is_available():    return "cuda"
        elif torch.backends.mps.is_available(): return "mps"
        return "cpu"
    
    @staticmethod
    def setup_device_specific_settings(device: str):
        if device == "cuda":
            use_bf16 = torch.cuda.is_bf16_supported()
            return not use_bf16, use_bf16  # fp16, bf16
        return False, False  # MPS/CPU: no mixed precision
```

### Memory Optimization Strategy
```python
# Training optimizations
model.config.use_cache = False              # Disable KV cache
model.gradient_checkpointing_enable()       # Trade compute for memory

# Data loading optimizations
DataCollatorForLanguageModeling(
    pad_to_multiple_of=8,    # Tensor core optimization
    return_tensors="pt"
)

# Training arguments
TrainingArguments(
    per_device_train_batch_size=1,      # Memory-safe default
    gradient_accumulation_steps=8,      # Maintain effective batch size
    dataloader_pin_memory=False,        # MPS compatibility
    fp16=use_fp16, bf16=use_bf16       # Device-specific precision
)
```

## ğŸ“ Data Flow Architecture

### Training Data Pipeline
```
Raw Dataset â†’ Alpaca Formatting â†’ Tokenization â†’ Batching â†’ Training
     â†“              â†“                  â†“           â†“          â†“
tatsu-lab/alpaca  Template         HF Tokenizer  Dynamic   PEFT Model
   [1% split]     Application      [truncation]  Padding   [LoRA/DoRA]
```

### Alpaca Prompt Template
```python
def format_alpaca_prompt(example):
    return f"""### Instruction:
{example['instruction']}
### Input:
{example.get('input', '')}
### Response:
{example['output']}"""
```

### Inference Data Flow
```
User Prompt â†’ Tokenization â†’ Model Forward â†’ Generation â†’ Decoding
     â†“             â†“              â†“             â†“          â†“
CLI Argument   HF Tokenizer   PEFT Model   Sampling   Text Output
               [padding]      [base+adapter] [temp/top_p] [detokenized]
```

## ğŸ”’ Error Handling Architecture

### Exception Hierarchy
```python
LlamaLoRAError (Base)
â”œâ”€â”€ ConfigurationError     # Invalid configuration
â”œâ”€â”€ ModelLoadingError     # Model/tokenizer loading failures  
â”œâ”€â”€ DatasetError         # Dataset loading/processing issues
â”œâ”€â”€ TrainingError        # Training execution problems
â”œâ”€â”€ AdapterError         # PEFT adapter operations
â””â”€â”€ ValidationError      # Runtime validation failures
```

### Error Context Preservation
```python
try:
    model = AutoModelForCausalLM.from_pretrained(model_id)
except Exception as e:
    raise ModelLoadingError(
        f"Failed to load model '{model_id}': {str(e)}"
    ) from e  # Preserves original traceback
```

## ğŸƒâ€â™‚ï¸ Performance Architecture

### Automatic Optimization Features
1. **Device Detection**: Optimal device selection with fallbacks
2. **Mixed Precision**: Automatic fp16/bf16 based on device capability
3. **Flash Attention**: FlashAttention2 with graceful degradation
4. **Memory Management**: Gradient checkpointing, dynamic padding
5. **Tensor Cores**: Batch size optimization for NVIDIA GPUs

### Scalability Design
- **Modular Components**: Each pipeline stage independently scalable
- **Configuration-Driven**: Easy parameter tuning without code changes
- **Memory Efficient**: Designed for constrained environments
- **Multi-Experiment**: Batch experiment execution with resource management

## ğŸ”„ Extensibility Points

### Adding New Models
1. Update `target_modules` in PEFT configuration
2. Modify tokenizer setup if needed
3. Add model-specific optimization flags

### Adding New Datasets
1. Implement dataset formatter in `TokenizerUtils`
2. Add dataset configuration in Pydantic schema
3. Update experiment presets

### Adding New PEFT Methods
1. Extend PEFT configuration in schema.py
2. Update model loading logic in train.py
3. Add validation rules in validate.py