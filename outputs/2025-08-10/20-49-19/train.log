[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Configuration Summary:
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - ==================================================
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Model: microsoft/DialoGPT-small
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Use DoRA: False
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Dataset: tatsu-lab/alpaca (train)
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Learning Rate: 2e-05
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Batch Size: 4
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - Epochs: 1
[2025-08-10 20:49:19,281][src.llama_lora.utils.common][INFO] - LoRA Rank: 16
[2025-08-10 20:49:19,282][src.llama_lora.utils.common][INFO] - Output Dir: ./output
[2025-08-10 20:49:19,282][src.llama_lora.utils.common][INFO] - ==================================================
[2025-08-10 20:49:19,282][src.llama_lora.utils.common][INFO] - Configuration validation successful!
[2025-08-10 20:49:19,292][src.llama_lora.utils.common][INFO] - Using device: mps
[2025-08-10 20:49:19,294][src.llama_lora.utils.common][INFO] - Loading tokenizer from 'microsoft/DialoGPT-small'...
[2025-08-10 20:49:19,883][src.llama_lora.utils.common][INFO] - Loading dataset tatsu-lab/alpaca...
[2025-08-10 20:49:22,604][src.llama_lora.utils.common][INFO] - Processing dataset...
[2025-08-10 20:49:26,566][src.llama_lora.utils.common][INFO] - Splitting dataset...
[2025-08-10 20:49:26,578][src.llama_lora.utils.common][INFO] - Dataset loaded: 49401 train, 2601 eval samples
[2025-08-10 20:49:26,578][src.llama_lora.utils.common][INFO] - Loading base model 'microsoft/DialoGPT-small'...
[2025-08-10 20:49:27,419][src.llama_lora.utils.common][INFO] - Setting up PEFT with LoRA/DoRA...
[2025-08-10 20:49:27,453][src.llama_lora.utils.common][INFO] - Trainable parameters:
[2025-08-10 20:49:27,668][src.llama_lora.utils.common][INFO] - Starting training...
