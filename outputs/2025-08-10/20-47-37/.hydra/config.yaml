model:
  model_id: microsoft/DialoGPT-small
  use_dora: false
  seq_len: 512
dataset:
  dataset_id: tatsu-lab/alpaca
  dataset_split: train
  val_ratio: 0.05
training:
  lr: 2.0e-05
  batch_size: 4
  gradient_accumulation_steps: 4
  epochs: 1
  seed: 42
  eval_steps: 50
  early_stopping_patience: 3
peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
  - q_proj
  - v_proj
output:
  base_output_dir: ./output
  adapter_dir: ./output/adapter
  tokenizer_dir: ./output/tokenizer
  merged_dir: ./output/merged
  log_dir: ./output/logs
logging:
  report_to: none
