{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "# # mid = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# mid = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mid)\n",
    "# model = AutoModelForCausalLM.from_pretrained(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*fp16.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*bf16.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "os.environ[\"ACCELERATE_USE_MPS\"] = \"true\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "USE_DORA = True\n",
    "SEQ_LEN = 1024\n",
    "LR = 2e-4\n",
    "BATCH = 1\n",
    "ACCUM = 8\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(e):\n",
    "    s = f\"### Instruction:\\n{e['instruction']}\\n### Input:\\n{e['input']}\\n### Response:\\n{e['output']}\"\n",
    "    return tok(s, truncation=True, max_length=SEQ_LEN)\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "model.to(device)\n",
    "model.config.use_cache = False\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    use_dora=USE_DORA,\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1%]\")\n",
    "\n",
    "tok_ds = dataset.map(fmt, remove_columns=dataset.column_names)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./out-tinyllama-lora\",\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    gradient_accumulation_steps=ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    fp16=False if device == \"mps\" else True,\n",
    "    bf16=False if device == \"mps\" else True,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./out-tinyllama-lora/adapter\")\n",
    "tok.save_pretrained(\"./out-tinyllama-lora/tokenizer\")\n",
    "print(\"Saved adapter to ./out-tinyllama-lora/adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "BASE = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "ADAPTER_DIR = \"./out-tinyllama-lora/adapter\"\n",
    "TOK_DIR = \"./out-tinyllama-lora/tokenizer\"\n",
    "\n",
    "tok_src = TOK_DIR if os.path.isdir(TOK_DIR) else BASE\n",
    "tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE)\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "model.to(device).eval()\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.1, do_sample=True):\n",
    "    ids = tok(prompt.strip(), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(generate(\"日本語で簡潔に答えて。富士山の標高は？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "BASE = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "ADAPTER_DIR = \"./out-tinyllama-lora/adapter\"\n",
    "TOK_DIR = \"./out-tinyllama-lora/tokenizer\"\n",
    "MERGED_DIR = \"./out-llama-lora/merged\"\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(TOK_DIR if os.path.isdir(TOK_DIR) else BASE, use_fast=True)\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE)\n",
    "peft_model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "\n",
    "# DoRA対応のpeftならOK\n",
    "merged = peft_model.merge_and_unload()\n",
    "\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tok.save_pretrained(MERGED_DIR)\n",
    "print(\"Merged model saved to:\", MERGED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cbde50a0644ffb9d653dfdff2fb83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827b432b4ca04a24b6ca77dbc2a50f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4962746ca6eb4d98a7c07a08d20863f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4dbea51f434ff8b0cde289c4c844c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ff29abc1de4b34b2e844cd368b55d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f427d1a9a6c8406ebf7bb61e36b24683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device).eval()\n",
    "\n",
    "\n",
    "def gen_base(prompt: str, max_new_tokens=128, temperature=0.7, top_p=0.9, do_sample=True, repetition_penalty=1.1):\n",
    "    ids = tok(prompt.strip(), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語で簡潔に答えて。富士山の標高は？ 10,722メートルです。\n",
      "\n",
      "そのとき、人類はそれが危険なものであると考えていました。したがって、20年間続けられた研究により、人類が富士山をたどるための道具や食料を開発することができました。山\n"
     ]
    }
   ],
   "source": [
    "print(gen_base(\"日本語で簡潔に答えて。富士山の標高は？\", max_new_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Aug 2025\n",
      "\n",
      "あなたは有能な日本語アシスタントです。事実に基づき簡潔に答えます。user\n",
      "\n",
      "富士山の標高は？assistant\n",
      "\n",
      "約3829メートルです。\n"
     ]
    }
   ],
   "source": [
    "def gen_chat(messages, max_new_tokens=128, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    # tokenizerにchatテンプレがある場合はそれを使う\n",
    "    if hasattr(tok, \"apply_chat_template\"):\n",
    "        text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        # 無い場合は簡易フォールバック\n",
    "        text = \"\"\n",
    "        for m in messages:\n",
    "            role = m[\"role\"].capitalize()\n",
    "            text += f\"[{role}]: {m['content']}\\n\"\n",
    "        text += \"[Assistant]: \"\n",
    "    return gen_base(text, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, do_sample=do_sample)\n",
    "\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは有能な日本語アシスタントです。事実に基づき簡潔に答えます。\"},\n",
    "    {\"role\": \"user\", \"content\": \"富士山の標高は？\"},\n",
    "]\n",
    "print(gen_chat(msgs, max_new_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "富士山の標高を日本語で簡潔に答えてください。\n",
      "### Input:\n",
      "\n",
      "### Response: \n",
      "\n",
      "富士山の標高は、約 2,500 メートルです。日本の最も高い山である。\n",
      "\n",
      "---\n",
      "\n",
      "### Instruction:\n",
      "富士山の標高を英語で説明してください。\n",
      "\n",
      "### Input:\n",
      "\n",
      "### Response: \n",
      "\n",
      "The highest mountain in Japan is Mount Fuji, which stands at an\n"
     ]
    }
   ],
   "source": [
    "alpaca = \"\"\"### Instruction:\n",
    "富士山の標高を日本語で簡潔に答えてください。\n",
    "### Input:\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "print(gen_base(alpaca, max_new_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "---- Plain ----\n",
      "日本語で簡潔に答えて。富士山の標高は？\n",
      "\n",
      "---- Chat template ----\n",
      "<|system|>\n",
      "あなたは事実に基づき日本語で簡潔に答えます。 \n",
      "<|user|>\n",
      "富士山の標高は？メートル単位で数値のみ。 \n",
      "<|assistant|>\n",
      "富士山の標高は、メートル単位で数値のみです。\n",
      "\n",
      "---- Alpaca template ----\n",
      "### Instruction:\n",
      "富士山の標高をメートル単位で数値のみで答えてください。\n",
      "### Input:\n",
      "\n",
      "### Response:\n",
      "1000.0\n",
      "\n",
      "Extracted number: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "MID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tok = AutoTokenizer.from_pretrained(MID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MID).to(device).eval()\n",
    "\n",
    "\n",
    "def generate(text, max_new_tokens=64, do_sample=False, temperature=None, top_p=None, repetition_penalty=1.0):\n",
    "    ids = tok(text, return_tensors=\"pt\").to(device)\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "    if do_sample and temperature is not None:\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "    if do_sample and top_p is not None:\n",
    "        gen_kwargs[\"top_p\"] = top_p\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, **gen_kwargs)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "plain = \"日本語で簡潔に答えて。富士山の標高は？\"\n",
    "print(\"---- Plain ----\")\n",
    "print(generate(plain))\n",
    "\n",
    "print(\"\\n---- Chat template ----\")\n",
    "if hasattr(tok, \"apply_chat_template\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"あなたは事実に基づき日本語で簡潔に答えます。\"},\n",
    "        {\"role\": \"user\", \"content\": \"富士山の標高は？メートル単位で数値のみ。\"},\n",
    "    ]\n",
    "    chat_text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "else:\n",
    "    chat_text = \"[System] あなたは事実に基づき日本語で簡潔に答えます。\\n[User] 富士山の標高は？メートル単位で数値のみ。\\n[Assistant] \"\n",
    "print(generate(chat_text))\n",
    "\n",
    "print(\"\\n---- Alpaca template ----\")\n",
    "alpaca = \"\"\"### Instruction:\n",
    "富士山の標高をメートル単位で数値のみで答えてください。\n",
    "### Input:\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "print(generate(alpaca))\n",
    "\n",
    "out = generate(alpaca)\n",
    "m = re.search(r\"\\d{3,5}\", out)\n",
    "print(\"\\nExtracted number:\", m.group(0) if m else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
